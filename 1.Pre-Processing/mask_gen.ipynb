{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5eb0ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import geopandas as gpd\n",
    "\n",
    "# gdf = gpd.read_file('/shared/data/climateplus2025/CapeTown_Image_2023_Training_1024_Oct.28/final_annotations_PV_all_types_5K_cleaned.gpkg')\n",
    "# # gdf = gpd.read_file('/home/cmn60/cape_town_segmentation/final_annotations_PV_all_types_balanced_3_cleaned.gpkg')\n",
    "\n",
    "# # Keep rows where at least one of the PV flags is 1\n",
    "# pv_mask = (gdf['PV_normal'] == 1) | (gdf['PV_heater'] == 1) | (gdf['PV_pool'] == 1)\n",
    "\n",
    "# # Exclude rows where uncertflag == 1\n",
    "# uncert_mask = gdf['uncertflag'] != 1\n",
    "# # Exclude rows where both PV_heater and PV_pool are 1, only 2 observations\n",
    "# heater_pool_overlap_mask = ~((gdf['PV_heater'] == 1) & (gdf['PV_pool'] == 1))\n",
    "\n",
    "\n",
    "# filtered_gdf = gdf[pv_mask & uncert_mask & heater_pool_overlap_mask].copy()\n",
    "\n",
    "# # Drop the 'uncertflag' column\n",
    "# if 'uncertflag' in filtered_gdf.columns:\n",
    "#     filtered_gdf = filtered_gdf.drop(columns=['uncertflag'])\n",
    "\n",
    "# # Output stats\n",
    "# print(f\"Filtered dataset contains {len(filtered_gdf)} PV-related arrays\")\n",
    "\n",
    "# # Save the filtered annotations\n",
    "# output_path = \"final_annotations_PV_all_types.gpkg\"\n",
    "# filtered_gdf.to_file(output_path, driver=\"GPKG\")\n",
    "# print(f\"Saved filtered annotations to {output_path}\")\n",
    "\n",
    "# # Show final column names\n",
    "# print(\"Remaining columns:\", filtered_gdf.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471a36a8",
   "metadata": {},
   "source": [
    "Converts PV annotation polygons from geographic coordinates (GeoPackage) into pixel coordinates that align with each corresponding GeoTIFF image, preparing them for image tiling and mask generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7d92299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid geometries found: 0\n",
      "Still invalid after fixing: 0\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "# Geometry error checker\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "gdf = gpd.read_file(\"final_annotations_PV_all_types_5K_cleaned.gpkg\")\n",
    "\n",
    "# Invalid geometry\n",
    "invalid_gdf = gdf[~gdf.geometry.is_valid].copy()\n",
    "invalid_gdf[\"image_name\"] = invalid_gdf[\"image_name\"]\n",
    "print(f\"Invalid geometries found: {len(invalid_gdf)}\")\n",
    "\n",
    "# Error correction : buffer(0) method \n",
    "gdf[\"geometry\"] = gdf[\"geometry\"].apply(lambda geom: geom.buffer(0) if not geom.is_valid else geom)\n",
    "\n",
    "\n",
    "still_invalid = gdf[~gdf.geometry.is_valid].copy()\n",
    "print(f\"Still invalid after fixing: {len(still_invalid)}\")\n",
    "\n",
    "gdf_valid = gdf[gdf.geometry.is_valid].copy()\n",
    "gdf_valid.to_file(\"final_annotations_PV_all_types_5K_cleaned.gpkg\", driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23d23717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "\n",
    "def convert_gpkg_to_pixel_coords(gpkg_path, image_dir):\n",
    "    gdf = gpd.read_file(gpkg_path)\n",
    "    pixel_rows = []\n",
    "\n",
    "    for image_name in gdf['image_name'].unique():\n",
    "        image_path = os.path.join(image_dir, f\"{image_name}.tif\")\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"Image not found: {image_path}\")\n",
    "            continue\n",
    "\n",
    "        with rasterio.open(image_path) as src:\n",
    "            transform = src.transform\n",
    "            inv_transform = ~transform\n",
    "            matrix = [inv_transform.a, inv_transform.b, inv_transform.d,\n",
    "                      inv_transform.e, inv_transform.xoff, inv_transform.yoff]\n",
    "\n",
    "            image_gdf = gdf[gdf['image_name'] == image_name]\n",
    "            for _, row in image_gdf.iterrows():\n",
    "                geom = row.geometry\n",
    "                if geom.is_empty or geom is None:\n",
    "                    continue\n",
    "\n",
    "                geom_px = affine_transform(geom, matrix)\n",
    "\n",
    "                if isinstance(geom_px, Polygon):\n",
    "                    polygons = [geom_px]\n",
    "                elif isinstance(geom_px, MultiPolygon):\n",
    "                    polygons = list(geom_px.geoms)\n",
    "                else:\n",
    "                    continue \n",
    "\n",
    "                for poly in polygons:\n",
    "                    centroid = poly.centroid\n",
    "                    pixel_rows.append({\n",
    "                        'image_name': image_name,\n",
    "                        'geometry': poly,\n",
    "                        'polygon_vertices_pixels': np.array(poly.exterior.coords),\n",
    "                        'centroid_latitude_pixels': centroid.y,\n",
    "                        'centroid_longitude_pixels': centroid.x,\n",
    "                        'PV_normal': row['PV_normal'],\n",
    "                        'PV_heater': row['PV_heater'],\n",
    "                        'PV_pool': row['PV_pool']\n",
    "                    })\n",
    "\n",
    "    return pd.DataFrame(pixel_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49318eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def create_multiclass_mask(image_shape, polygons_with_classes):\n",
    "    mask = np.zeros(image_shape[:2], dtype=\"uint8\")\n",
    "    for polygon, class_id in polygons_with_classes:\n",
    "        cv2.fillPoly(mask, [polygon.astype(np.int32)], class_id)\n",
    "    return mask\n",
    "\n",
    "def adjust_polygon_coordinates(polygons, x_offset, y_offset):\n",
    "    return [(poly - np.array([x_offset, y_offset]), cls) for poly, cls in polygons]\n",
    "\n",
    "def save_tile_and_mask(tile, mask, tile_index_pixels, tile_dir, mask_dir, image_name):\n",
    "    tile_path = os.path.join(tile_dir, f\"i_{image_name}_{tile_index_pixels}.png\")\n",
    "    mask_path = os.path.join(mask_dir, f\"m_{image_name}_{tile_index_pixels}.png\")\n",
    "    cv2.imwrite(tile_path, cv2.cvtColor(tile, cv2.COLOR_RGB2BGR))\n",
    "    cv2.imwrite(mask_path, mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86a3d2d",
   "metadata": {},
   "source": [
    "3Classes: PV_normal = 1, PV_heater = 2, PV_pool = 3 \n",
    "    * Note : Background = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab67334e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_geotiff(image_name, image_path, tile_size, df, tile_dir, mask_dir):\n",
    "    with rasterio.open(image_path) as src:\n",
    "        img = np.transpose(src.read(), (1, 2, 0))\n",
    "        h, w = img.shape[:2]\n",
    "\n",
    "        pad_h = (tile_size - h % tile_size) % tile_size\n",
    "        pad_w = (tile_size - w % tile_size) % tile_size\n",
    "        padded = np.pad(img, ((0, pad_h), (0, pad_w), (0, 0)), mode='constant')\n",
    "\n",
    "        for y in range(0, padded.shape[0], tile_size):\n",
    "            for x in range(0, padded.shape[1], tile_size):\n",
    "                tile = padded[y:y+tile_size, x:x+tile_size]\n",
    "                polygons_in_tile = []\n",
    "\n",
    "                for _, row in df.iterrows():\n",
    "                    cx, cy = row['centroid_longitude_pixels'], row['centroid_latitude_pixels']\n",
    "                    if x <= cx < x + tile_size and y <= cy < y + tile_size:\n",
    "                        poly = row['polygon_vertices_pixels']\n",
    "                        if row['PV_normal'] == 1:\n",
    "                            cls = 1\n",
    "                        elif row['PV_heater'] == 1:\n",
    "                            cls = 2\n",
    "                        elif row['PV_pool'] == 1:\n",
    "                            cls = 3\n",
    "                        else:\n",
    "                            continue\n",
    "                        polygons_in_tile.append((poly, cls))\n",
    "\n",
    "                adj_polygons = adjust_polygon_coordinates(polygons_in_tile, x, y)\n",
    "                mask = create_multiclass_mask(tile.shape, adj_polygons)\n",
    "\n",
    "                if np.any(mask > 0):\n",
    "                    tile_idx = f\"{y//tile_size}_{x//tile_size}\"\n",
    "                    save_tile_and_mask(tile, mask, tile_idx, tile_dir, mask_dir, image_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eca06204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_images(image_dir, annotations_df, tile_size, tile_dir, mask_dir):\n",
    "    os.makedirs(tile_dir, exist_ok=True)\n",
    "    os.makedirs(mask_dir, exist_ok=True)\n",
    "\n",
    "    for i, image_name in enumerate(annotations_df['image_name'].unique()):\n",
    "        print(f\"[{i}] Processing {image_name}\")\n",
    "        img_path = os.path.join(image_dir, f\"{image_name}.tif\")\n",
    "        if not os.path.exists(img_path):\n",
    "            continue\n",
    "        df_img = annotations_df[annotations_df['image_name'] == image_name]\n",
    "        process_geotiff(image_name, img_path, tile_size, df_img, tile_dir, mask_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0b4c210",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def copy_masks_with_target(src_folder, dst_folder):\n",
    "    os.makedirs(dst_folder, exist_ok=True)\n",
    "    for f in os.listdir(src_folder):\n",
    "        mask = cv2.imread(os.path.join(src_folder, f), cv2.IMREAD_GRAYSCALE)\n",
    "        if np.any(mask > 0):\n",
    "            shutil.copy(os.path.join(src_folder, f), dst_folder)\n",
    "\n",
    "def copy_corresponding_images(mask_folder, image_folder, dst_folder):\n",
    "    os.makedirs(dst_folder, exist_ok=True)\n",
    "    for f in os.listdir(mask_folder):\n",
    "        img_f = \"i\" + f[1:]\n",
    "        src = os.path.join(image_folder, img_f)\n",
    "        if os.path.exists(src):\n",
    "            shutil.copy(src, dst_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e979ebb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] Processing 2023_RGB_8cm_W57B_10\n",
      "[1] Processing 2023_RGB_8cm_W57B_4\n",
      "[2] Processing 2023_RGB_8cm_W57B_15\n",
      "[3] Processing 2023_RGB_8cm_W53B_24\n",
      "[4] Processing 2023_RGB_8cm_W36B_15\n",
      "[5] Processing 2023_RGB_8cm_W16C_20\n",
      "[6] Processing 2023_RGB_8cm_W57B_13\n",
      "[7] Processing 2023_RGB_8cm_W53B_9\n",
      "[8] Processing 2023_RGB_8cm_W25A_9\n",
      "[9] Processing 2023_RGB_8cm_W17B_3\n",
      "[10] Processing 2023_RGB_8cm_W36B_20\n",
      "[11] Processing 2023_RGB_8cm_W17B_11\n",
      "[12] Processing 2023_RGB_8cm_W57C_5\n",
      "[13] Processing 2023_RGB_8cm_W17B_6\n",
      "[14] Processing 2023_RGB_8cm_W17B_7\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from shapely.affinity import affine_transform\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "gpkg_path = \"/shared/data/climateplus2025/CapeTown_Image_2023_Training_1024_Oct.28/final_annotations_PV_all_types_5K_cleaned.gpkg\"\n",
    "image_dir = \"/shared/data/climateplus2025/CapeTown_Image_2023_Training_1024_Oct.28/Images_original\" #.tif files\n",
    "tile_dir = \"tiles_1024_5k\"\n",
    "mask_dir = \"/shared/data/climateplus2025/CapeTown_Image_2023_Training_1024_Oct.28/Images_masks/masks_1024_5k\"\n",
    "target_mask_dir = \"masks_target_5k\"\n",
    "target_image_dir = \"images_target_5k\"\n",
    "tile_size = 1024\n",
    "\n",
    "annotations_df = convert_gpkg_to_pixel_coords(gpkg_path, image_dir)\n",
    "\n",
    "process_all_images(image_dir, annotations_df, tile_size, tile_dir, mask_dir)\n",
    "\n",
    "copy_masks_with_target(mask_dir, target_mask_dir)\n",
    "copy_corresponding_images(target_mask_dir, tile_dir, target_image_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060bc124",
   "metadata": {},
   "source": [
    "Dataset Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575e2e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Unstratified dataset split: train/val/test (see below for stratified)\n",
    "# import os\n",
    "# import shutil\n",
    "# import random\n",
    "\n",
    "# # Set your input directories\n",
    "# images_dir = '/shared/data/climateplus2025/CapeTown_Image_2023_Training_1024_Oct.28/images_target_5k'\n",
    "# masks_dir = '/shared/data/climateplus2025/CapeTown_Image_2023_Training_1024_Oct.28/masks_target_5k'\n",
    "\n",
    "# # Set your output base directory\n",
    "# output_dir = '/shared/data/climateplus2025/CapeTown_Image_2023_Training_1024_Oct.28/Dataset_split_output5k'\n",
    "# image_files = [f for f in os.listdir(images_dir) if f.endswith(('.png', '.jpg', '.tif'))]\n",
    "# image_suffixes = [f[2:] for f in image_files]  # Remove 'i_' prefix\n",
    "\n",
    "# # Shuffle\n",
    "# random.shuffle(image_suffixes)\n",
    "\n",
    "# # Split\n",
    "# total = len(image_suffixes)\n",
    "# train_end = int(0.7 * total)\n",
    "# val_end = train_end + int(0.15 * total)\n",
    "\n",
    "# splits = {\n",
    "#     'train': image_suffixes[:train_end],\n",
    "#     'val': image_suffixes[train_end:val_end],\n",
    "#     'test': image_suffixes[val_end:]\n",
    "# }\n",
    "\n",
    "# # Create directories\n",
    "# for split in splits:\n",
    "#     os.makedirs(os.path.join(output_dir, split, 'images'), exist_ok=True)\n",
    "#     os.makedirs(os.path.join(output_dir, split, 'masks'), exist_ok=True)\n",
    "\n",
    "# # Copy files\n",
    "# for split_name, suffix_list in splits.items():\n",
    "#     for suffix in suffix_list:\n",
    "#         image_file = f\"i_{suffix}\"\n",
    "#         mask_file = f\"m_{suffix}\"\n",
    "        \n",
    "#         src_image = os.path.join(images_dir, image_file)\n",
    "#         src_mask = os.path.join(masks_dir, mask_file)\n",
    "#         dst_image = os.path.join(output_dir, split_name, 'images', image_file)\n",
    "#         dst_mask = os.path.join(output_dir, split_name, 'masks', mask_file)\n",
    "        \n",
    "#         if os.path.exists(src_image) and os.path.exists(src_mask):\n",
    "#             shutil.copy(src_image, dst_image)\n",
    "#             shutil.copy(src_mask, dst_mask)\n",
    "#         else:\n",
    "#             print(f\"Warning: Missing pair for {suffix}\")\n",
    "\n",
    "# print(\"Dataset split completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497f197a",
   "metadata": {},
   "source": [
    "**Stratified Sampling**\n",
    "\n",
    "Multi-label per tile - Stratify by treating the full set of classes per tile as a single unit (i.e., a class combination). This ensures the split preserves the distribution of tiles with specific class combos, instead of simplifying to a dominant class.\n",
    "\n",
    "store each tileâ€™s ID and its corresponding class combination to define class distribution across tiles(=pathces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5671c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/sklearn/utils/fixes.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import parse_version  # type: ignore\n"
     ]
    }
   ],
   "source": [
    "# Stratified dataset split\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "images_dir = '/shared/data/climateplus2025/CapeTown_Image_2023_Training_1024_Oct.28/images_target_5k'\n",
    "masks_dir = '/shared/data/climateplus2025/CapeTown_Image_2023_Training_1024_Oct.28/masks_target_5k'\n",
    "output_dir = '/shared/data/climateplus2025/CapeTown_Image_2023_Training_1024_Oct.28/output5k_stratified'\n",
    "\n",
    "# Step 1: Load all mask files and assign class combination label\n",
    "data = []\n",
    "for fname in os.listdir(masks_dir):\n",
    "    if fname.endswith('.png'):\n",
    "        mask = cv2.imread(os.path.join(masks_dir, fname), cv2.IMREAD_GRAYSCALE)\n",
    "        classes = set(np.unique(mask)) - {0}  # exclude background\n",
    "\n",
    "        combo = f\"{int(1 in classes)}_{int(2 in classes)}_{int(3 in classes)}\"\n",
    "        suffix = fname[2:]  # strip 'm_' prefix\n",
    "        data.append({'suffix': suffix, 'combo': combo})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 2: Stratified split\n",
    "train_val_df, test_df = train_test_split(df, test_size=0.10, stratify=df['combo'], random_state=42)\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.1111, stratify=train_val_df['combo'], random_state=42)\n",
    "\n",
    "splits = {\n",
    "    'train': train_df['suffix'].tolist(),\n",
    "    'val': val_df['suffix'].tolist(),\n",
    "    'test': test_df['suffix'].tolist()\n",
    "}\n",
    "\n",
    "# Step 3: Copy files to output structure\n",
    "import shutil\n",
    "\n",
    "for split in splits:\n",
    "    os.makedirs(os.path.join(output_dir, split, 'images'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, split, 'masks'), exist_ok=True)\n",
    "\n",
    "for split_name, suffixes in splits.items():\n",
    "    for suffix in suffixes:\n",
    "        img_file = f\"i_{suffix}\"\n",
    "        mask_file = f\"m_{suffix}\"\n",
    "\n",
    "        src_img = os.path.join(images_dir, img_file)\n",
    "        src_mask = os.path.join(masks_dir, mask_file)\n",
    "        dst_img = os.path.join(output_dir, split_name, 'images', img_file)\n",
    "        dst_mask = os.path.join(output_dir, split_name, 'masks', mask_file)\n",
    "\n",
    "        if os.path.exists(src_img) and os.path.exists(src_mask):\n",
    "            shutil.copy(src_img, dst_img)\n",
    "            shutil.copy(src_mask, dst_mask)\n",
    "        else:\n",
    "            print(f\"âš ï¸ Missing pair for {suffix}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9ac60f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ TRAIN combo distribution:\n",
      "combo\n",
      "0_1_0    200\n",
      "1_1_1    169\n",
      "1_0_0    147\n",
      "1_1_0    116\n",
      "0_1_1    113\n",
      "0_0_1    111\n",
      "1_0_1     84\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ðŸ”¹ VAL combo distribution:\n",
      "combo\n",
      "0_1_0    25\n",
      "1_1_1    21\n",
      "1_0_0    19\n",
      "0_0_1    14\n",
      "1_1_0    14\n",
      "0_1_1    14\n",
      "1_0_1    11\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ðŸ”¹ TEST combo distribution:\n",
      "combo\n",
      "0_1_0    25\n",
      "1_1_1    21\n",
      "1_0_0    19\n",
      "0_1_1    14\n",
      "1_1_0    14\n",
      "0_0_1    14\n",
      "1_0_1    11\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print dataset splitting\n",
    "# 0_1_0 means the image has (no) PV normal, (yes) PV heater, and (no) PV pool\n",
    "for split_name, suffix_list in splits.items():\n",
    "    split_df = df[df['suffix'].isin(suffix_list)]\n",
    "    print(f\"\\nðŸ”¹ {split_name.upper()} combo distribution:\")\n",
    "    print(split_df['combo'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd5e6fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PV_normal: 2033\n",
      "PV_heater: 1983\n",
      "PV_pool: 1146\n"
     ]
    }
   ],
   "source": [
    "# Count and print number of annotations for each PV type\n",
    "num_normal = (gdf_valid['PV_normal'] == 1).sum()\n",
    "num_heater = (gdf_valid['PV_heater'] == 1).sum()\n",
    "num_pool = (gdf_valid['PV_pool'] == 1).sum()\n",
    "\n",
    "print(f\"PV_normal: {num_normal}\")\n",
    "print(f\"PV_heater: {num_heater}\")\n",
    "print(f\"PV_pool: {num_pool}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
